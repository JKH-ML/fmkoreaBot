import sys
import os
import time
import requests
import json
from bs4 import BeautifulSoup
from seleniumbase import SB
from urllib.parse import urlparse, parse_qs

# Windowsì—ì„œ í•œêµ­ì–´ ì¶œë ¥ì„ ìœ„í•´ ì¸ì½”ë”© ì„¤ì •
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')

DATA_FILE = "data.json"

def load_data():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_data(data):
    # ìµœëŒ€ 100ê°œ ìœ ì§€ (ì˜¤ë˜ëœ í•­ëª© ì‚­ì œ)
    if len(data) > 100:
        data = data[-100:]
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def extract_id(url):
    parsed_url = urlparse(url)
    qs = parse_qs(parsed_url.query)
    # FMKorea URL format: ...document_srl=12345...
    return qs.get("document_srl", [None])[0]

def send_discord_message(webhook_url, posts):
    if not webhook_url or not posts:
        return

    # ë””ìŠ¤ì½”ë“œ ì„ë² ë“œëŠ” í•œ ë²ˆì— ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ê°€ëŠ¥
    # 10ê°œê°€ ë„˜ìœ¼ë©´ ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ ì„œ ë³´ëƒ„
    for i in range(0, len(posts), 10):
        chunk = posts[i:i+10]
        embeds = []
        for post in chunk:
            embeds.append({
                "title": f"ğŸ”¥ [ì¶”ì²œ: {post['count']}] {post['title']}",
                "url": post['link'],
                "color": 15548997
            })

        payload = {
            "content": "ğŸ“¢ **ìƒˆë¡œìš´ ì¸ê¸° ê²Œì‹œê¸€ ì•Œë¦¼ (ì¶”ì²œ 300+ ê±´)**" if i == 0 else "",
            "embeds": embeds
        }

        try:
            requests.post(webhook_url, json=payload)
        except Exception as e:
            print(f"ë””ìŠ¤ì½”ë“œ ì „ì†¡ ì˜¤ë¥˜: {e}")

def get_top_posts():
    base_url = "https://www.fmkorea.com/index.php?mid=afreecatv&sort_index=pop&order_type=desc&page={}"
    webhook_url = os.environ.get("DISCORD_WEBHOOK")
    
    processed_ids = load_data()
    new_posts = []
    new_ids = []

    print("ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ì¶”ì²œ 300 ì´ìƒ í•„í„°ë§)...")
    
    with SB(uc=True, test=True, headless=False, locale_code="ko") as sb:
        for page_num in range(1, 6):
            url = base_url.format(page_num)
            try:
                print(f"{page_num}í˜ì´ì§€ ì ‘ì† ì¤‘...")
                sb.uc_open_with_reconnect(url, 4)
                time.sleep(3)
                
                html = sb.get_page_source()
                soup = BeautifulSoup(html, 'html.parser')
                posts = soup.select('li.li')
                
                found_new_on_page = 0
                for post in posts:
                    title_elem = post.select_one('h3.title span.ellipsis-target') or post.select_one('h3.title a')
                    count_elem = post.select_one('span.count')
                    link_elem = post.select_one('h3.title a')
                    
                    if title_elem and count_elem and link_elem:
                        try:
                            count_text = count_elem.get_text(strip=True)
                            count = int(''.join(filter(str.isdigit, count_text)))
                        except:
                            count = 0
                        
                        # ì¶”ì²œìˆ˜ 300 ì´ìƒ í•„í„°
                        if count < 300:
                            continue

                        link = link_elem['href']
                        if not link.startswith('http'):
                            link = "https://www.fmkorea.com" + link
                        
                        doc_id = extract_id(link)
                        # ì¤‘ë³µ ì²´í¬ (ì´ë¯¸ ì²˜ë¦¬ëœ IDì´ê±°ë‚˜ í˜„ì¬ ë£¨í”„ì—ì„œ ìƒˆë¡œ ë°œê²¬ëœ IDì¸ ê²½ìš° ì œì™¸)
                        if not doc_id or doc_id in processed_ids or doc_id in new_ids:
                            continue

                        title = " ".join(title_elem.get_text().split())
                        new_posts.append({'title': title, 'count': count, 'link': link})
                        new_ids.append(doc_id)
                        found_new_on_page += 1
                
                print(f"{page_num}í˜ì´ì§€ ì™„ë£Œ (ìƒˆë¡œìš´ ê¸€: {found_new_on_page})")
                
            except Exception as e:
                print(f"{page_num}í˜ì´ì§€ ì˜¤ë¥˜: {e}")
                continue

    if new_posts:
        print(f"ìƒˆë¡œìš´ ê²Œì‹œê¸€ {len(new_posts)}ê°œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ë””ìŠ¤ì½”ë“œë¡œ ì „ì†¡í•©ë‹ˆë‹¤.")
        # ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì €ì¥
        processed_ids.extend(new_ids)
        save_data(processed_ids)
        # ë””ìŠ¤ì½”ë“œ ì „ì†¡
        send_discord_message(webhook_url, new_posts)
    else:
        print("ìƒˆë¡œ ì¶”ê°€ëœ ì¡°ê±´(300+)ì— ë§ëŠ” ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    get_top_posts()
