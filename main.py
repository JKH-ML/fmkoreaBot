import sys
import os
import time
import requests
import json
from bs4 import BeautifulSoup
from seleniumbase import SB
from urllib.parse import urlparse, parse_qs

# Windowsì—ì„œ í•œêµ­ì–´ ì¶œë ¥ì„ ìœ„í•´ ì¸ì½”ë”© ì„¤ì •
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')

DATA_FILE = "data.json"

def load_data():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data if isinstance(data, list) else []
        except:
            return []
    return []

def save_data(data):
    # ìµœëŒ€ 100ê°œ ìœ ì§€ (ì˜¤ëž˜ëœ í•­ëª© ì‚­ì œ)
    if len(data) > 100:
        data = data[-100:]
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def extract_id(url):
    parsed_url = urlparse(url)
    qs = parse_qs(parsed_url.query)
    return qs.get("document_srl", [None])[0]

def send_discord_message(webhook_url, posts):
    if not webhook_url or not posts:
        return

    for i in range(0, len(posts), 10):
        chunk = posts[i:i+10]
        embeds = []
        for post in chunk:
            embeds.append({
                "title": f"ðŸ”¥ [ì¶”ì²œ: {post['count']}] {post['title']}",
                "url": post['link'],
                "color": 15548997
            })

        payload = {
            "content": "ðŸ“¢ **ìƒˆë¡œìš´ ì¸ê¸° ê²Œì‹œê¸€ ì•Œë¦¼ (ì¶”ì²œ 300+ ê±´)**" if i == 0 else "",
            "embeds": embeds
        }
        try:
            requests.post(webhook_url, json=payload)
        except Exception as e:
            print(f"ë””ìŠ¤ì½”ë“œ ì „ì†¡ ì˜¤ë¥˜: {e}")

def get_top_posts():
    base_url = "https://www.fmkorea.com/index.php?mid=afreecatv&sort_index=pop&order_type=desc&page={}"
    webhook_url = os.environ.get("DISCORD_WEBHOOK")
    
    processed_ids = load_data()
    new_posts = []
    new_ids = []

    print(f"ìˆ˜ì§‘ì„ ì‹œìž‘í•©ë‹ˆë‹¤ (í˜„ìž¬ ì €ìž¥ëœ ê¸€ ë²ˆí˜¸: {len(processed_ids)}ê°œ)")
    
    # uc=Trueë¡œ ì„¤ì •í•˜ì—¬ undetected ëª¨ë“œ ì‚¬ìš©
    with SB(uc=True, test=True, headless=False, locale_code="ko") as sb:
        for page_num in range(1, 6):
            url = base_url.format(page_num)
            try:
                print(f"\n[{page_num}íŽ˜ì´ì§€ ì ‘ì† ì¤‘...]")
                sb.uc_open_with_reconnect(url, 4)
                time.sleep(5) # íŽ˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¶©ë¶„ížˆ í™•ë³´
                
                # Cloudflare ì²´í¬
                title = sb.get_title()
                print(f"íŽ˜ì´ì§€ íƒ€ì´í‹€: {title}")
                
                if "Just a moment" in title or "Cloudflare" in title:
                    print("ë³´ì•ˆ í™•ì¸ íŽ˜ì´ì§€ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìš°íšŒë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                    sb.uc_gui_click_captcha()
                    time.sleep(5)
                
                html = sb.get_page_source()
                soup = BeautifulSoup(html, 'html.parser')
                posts = soup.select('li.li')
                
                total_on_page = len(posts)
                max_count_on_page = 0
                found_new_on_page = 0
                
                if total_on_page == 0:
                    print("ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦°ìƒ·ì„ ì €ìž¥í•©ë‹ˆë‹¤.")
                    sb.save_screenshot(f"debug_page_{page_num}.png")
                
                for post in posts:
                    title_elem = post.select_one('h3.title span.ellipsis-target') or post.select_one('h3.title a')
                    count_elem = post.select_one('span.count')
                    link_elem = post.select_one('h3.title a')
                    
                    if title_elem and count_elem and link_elem:
                        try:
                            count_text = count_elem.get_text(strip=True)
                            count = int(''.join(filter(str.isdigit, count_text)))
                        except:
                            count = 0
                        
                        if count > max_count_on_page:
                            max_count_on_page = count
                        
                        if count < 300:
                            continue

                        link = link_elem['href']
                        if not link.startswith('http'):
                            link = "https://www.fmkorea.com" + link
                        
                        doc_id = extract_id(link)
                        if not doc_id or doc_id in processed_ids or doc_id in new_ids:
                            continue

                        title = " ".join(title_elem.get_text().split())
                        new_posts.append({'title': title, 'count': count, 'link': link})
                        new_ids.append(doc_id)
                        found_new_on_page += 1
                
                print(f"ê²°ê³¼: ì „ì²´ {total_on_page}ê°œ ì¤‘ ìµœê³  ì¶”ì²œìˆ˜ {max_count_on_page}, ìƒˆë¡œ ì¶”ê°€ëœ ê¸€ {found_new_on_page}ê°œ")
                
            except Exception as e:
                print(f"{page_num}íŽ˜ì´ì§€ ì˜¤ë¥˜: {e}")
                continue

    if new_posts:
        print(f"\nì´ {len(new_posts)}ê°œì˜ ìƒˆë¡œìš´ ê²Œì‹œê¸€ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        processed_ids.extend(new_ids)
        save_data(processed_ids)
        send_discord_message(webhook_url, new_posts)
    else:
        print("\nìƒˆë¡œ ì¶”ê°€í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    get_top_posts()
